{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b9573e-229f-4425-9ceb-cfdd4f1aadb9",
   "metadata": {},
   "source": [
    "## 注意：大量代码是冗余的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "869a3737-7f45-4785-b36a-d5e9401fbfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300}\n",
      "      serial_number        collect_time\n",
      "761    server_10807 2019-06-03 08:05:00\n",
      "903    server_10961 2019-06-09 00:10:00\n",
      "905    server_10961 2019-06-09 00:20:00\n",
      "906    server_10961 2019-06-09 00:25:00\n",
      "907    server_10961 2019-06-09 00:30:00\n",
      "...             ...                 ...\n",
      "54688   server_9924 2019-06-01 15:10:00\n",
      "54689   server_9924 2019-06-04 14:30:00\n",
      "54694   server_9924 2019-06-10 13:05:00\n",
      "54695   server_9924 2019-06-11 12:50:00\n",
      "54697   server_9924 2019-06-13 12:25:00\n",
      "\n",
      "[520 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "kernel_log_data_path = r'memory_sample_kernel_log_round1_a_train.csv'\n",
    "failure_tag_data_path = r'memory_sample_failure_tag_round1_a_train.csv'\n",
    "PARENT_FOLDER = 'data'\n",
    "\n",
    "# 计算每个agg_time区间的和\n",
    "def etl(path, agg_time):\n",
    "    data = pd.read_csv(os.path.join(PARENT_FOLDER, path))\n",
    "    # 检查缺失值\n",
    "    if data.isnull().sum().sum() > 0:\n",
    "        data = data.fillna(method='ffill')  # 填充缺失值\n",
    "    # 降低时间精度 向上取整\n",
    "    data['collect_time'] = pd.to_datetime(data['collect_time']).dt.ceil(agg_time)\n",
    "    group_data = data.groupby(['serial_number', 'collect_time'], as_index=False).agg('sum')\n",
    "    return group_data\n",
    "\n",
    "# 设置聚合时间粒度\n",
    "AGG_VALUE = 5\n",
    "AGG_UNIT = 'min'\n",
    "AGG_TIME = str(AGG_VALUE) + AGG_UNIT\n",
    "\n",
    "# 示例仅使用了kernel数据\n",
    "group_min = etl(kernel_log_data_path, AGG_TIME)\n",
    "failure_tag = pd.read_csv(os.path.join(PARENT_FOLDER, failure_tag_data_path))\n",
    "failure_tag['failure_time'] = pd.to_datetime(failure_tag['failure_time'])\n",
    "\n",
    "# 为数据打标\n",
    "merged_data = pd.merge(group_min, failure_tag[['serial_number', 'failure_time']], how='left', on=['serial_number'])\n",
    "merged_data['failure_tag'] = (merged_data['failure_time'].notnull()) & ((merged_data['failure_time']\n",
    "                                                                         - merged_data['collect_time']).dt.seconds <= AGG_VALUE * 60)\n",
    "merged_data['failure_tag'] = merged_data['failure_tag'] + 0\n",
    "feature_data = merged_data.drop(['serial_number', 'collect_time', 'manufacturer', 'vendor', 'failure_time'], axis=1)\n",
    "\n",
    "# 计算正负样本的数量\n",
    "num_positive_samples = len(feature_data[feature_data['failure_tag'] == 1])\n",
    "num_negative_samples = len(feature_data[feature_data['failure_tag'] == 0])\n",
    "\n",
    "# 根据正样本的数量决定上采样倍数\n",
    "upsampling_factor = num_positive_samples // num_negative_samples\n",
    "\n",
    "# 负样本上采样\n",
    "sampled_negative_data = feature_data[feature_data['failure_tag'] == 0].sample(n=upsampling_factor, replace=True)\n",
    "\n",
    "# 合并正负样本\n",
    "upsampled_data = pd.concat([sampled_negative_data, feature_data[feature_data['failure_tag'] == 1]])\n",
    "\n",
    "# 重新采样后的数据集\n",
    "sample = upsampled_data\n",
    "\n",
    "# 将负样本的类别标签从1改为0\n",
    "upsampled_data['failure_tag'] = upsampled_data['failure_tag'].replace({1: 0})\n",
    "\n",
    "# 负样本下采样\n",
    "sample_0 = feature_data[feature_data['failure_tag'] == 0].sample(frac=0.02)\n",
    "sample = pd.concat([sample_0, feature_data[feature_data['failure_tag'] == 1]])\n",
    "\n",
    "# 计算异常值得分，将异常值得分作为特征，注意异常值检测特征的位置\n",
    "clf = IsolationForest(contamination=0.05)  # 设置异常值比例\n",
    "merged_data['outlier_score'] = clf.fit_predict(feature_data.iloc[:, :-1])\n",
    "\n",
    "# 时间特征：提取更多时间特征，如小时、分钟、周几等。\n",
    "merged_data['hour'] = merged_data['collect_time'].dt.hour\n",
    "merged_data['day_of_week'] = merged_data['collect_time'].dt.dayofweek\n",
    "\n",
    "# 将异常值得分作为特征\n",
    "feature_data['outlier_score'] = merged_data['outlier_score']\n",
    "\n",
    "# 在计算特征后插入异常值检测特征工程\n",
    "clf = IsolationForest(contamination=0.05)  # 设置异常值比例\n",
    "merged_data['outlier_score'] = clf.fit_predict(feature_data.iloc[:, :-1])\n",
    "\n",
    "# 将异常值得分作为特征\n",
    "feature_data['outlier_score'] = merged_data['outlier_score']\n",
    "\n",
    "# 过去故障次数\n",
    "merged_data['past_failure_count'] = merged_data.groupby('serial_number')['failure_tag'].cumsum().shift()\n",
    "\n",
    "# 负样本上采样   #添加一个网格搜索代码\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(sample.iloc[:, :-1], sample['failure_tag'])\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# 使用最佳参数训练模型\n",
    "clf = XGBClassifier(**best_params)\n",
    "clf.fit(sample.iloc[:, :-1], sample['failure_tag'])\n",
    "\n",
    "# 测试数据\n",
    "group_data_test = etl(r'memory_sample_kernel_log_k12_round1_a_test.csv', AGG_TIME)\n",
    "group_min_sn_test = pd.DataFrame(group_data_test[['serial_number', 'collect_time']])\n",
    "group_min_test = group_data_test.drop(['serial_number', 'collect_time', 'manufacturer', 'vendor'], axis=1)\n",
    "\n",
    "# 模型预测\n",
    "res = clf.predict(group_min_test)\n",
    "group_min_sn_test['predict'] = res\n",
    "\n",
    "# 保存结果\n",
    "group_min_sn_test = group_min_sn_test[group_min_sn_test['predict'] == 1]\n",
    "group_min_sn_res = group_min_sn_test.drop('predict', axis=1)\n",
    "\n",
    "output_path = os.path.join('./', 'memory_predit_res_svm.csv')\n",
    "print(group_min_sn_res)\n",
    "\n",
    "# 由于预测pti对分数影响不大，先直接末尾增加pti为1\n",
    "pti = 5\n",
    "with open(output_path, 'w') as result_fp:\n",
    "    for _, _row in group_min_sn_res.iterrows():\n",
    "        result_fp.write(\"{},{},{}\\n\".format(_row.serial_number, _row.collect_time, pti))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20214d63-eae8-48f6-8314-70e32867e7d7",
   "metadata": {},
   "source": [
    "## 分段说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f774e-6156-4fe7-847a-fd1f301b2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "kernel_log_data_path = r'memory_sample_kernel_log_round1_a_train.csv'\n",
    "failure_tag_data_path = r'memory_sample_failure_tag_round1_a_train.csv'\n",
    "PARENT_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa8153-c27e-46d1-8282-0770c06ff98e",
   "metadata": {},
   "source": [
    "**NOTE**：加入前向填充(这应该是整个代码文件中唯一一个有效的，你也可以尝试其他代码)\n",
    "\n",
    "**一定要先进行前向填充再进行数据聚合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f153f-2138-41d6-8f0c-edddd6dd57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个agg_time区间的和\n",
    "def etl(path, agg_time):\n",
    "    data = pd.read_csv(os.path.join(PARENT_FOLDER, path))\n",
    "    # 检查缺失值\n",
    "    if data.isnull().sum().sum() > 0:\n",
    "        data = data.fillna(method='ffill')  # 填充缺失值\n",
    "    # 降低时间精度 向上取整\n",
    "    data['collect_time'] = pd.to_datetime(data['collect_time']).dt.ceil(agg_time)\n",
    "    group_data = data.groupby(['serial_number', 'collect_time'], as_index=False).agg('sum')\n",
    "    return group_data\n",
    "\n",
    "# 设置聚合时间粒度\n",
    "AGG_VALUE = 5\n",
    "AGG_UNIT = 'min'\n",
    "AGG_TIME = str(AGG_VALUE) + AGG_UNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb20f1-36b6-4678-bc6c-d10ac69aaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例仅使用了kernel数据\n",
    "group_min = etl(kernel_log_data_path, AGG_TIME)\n",
    "failure_tag = pd.read_csv(os.path.join(PARENT_FOLDER, failure_tag_data_path))\n",
    "failure_tag['failure_time'] = pd.to_datetime(failure_tag['failure_time'])\n",
    "\n",
    "# 为数据打标\n",
    "merged_data = pd.merge(group_min, failure_tag[['serial_number', 'failure_time']], how='left', on=['serial_number'])\n",
    "merged_data['failure_tag'] = (merged_data['failure_time'].notnull()) & ((merged_data['failure_time']\n",
    "                                                                         - merged_data['collect_time']).dt.seconds <= AGG_VALUE * 60)\n",
    "merged_data['failure_tag'] = merged_data['failure_tag'] + 0\n",
    "feature_data = merged_data.drop(['serial_number', 'collect_time', 'manufacturer', 'vendor', 'failure_time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aaf54-88fb-4045-92cf-801de7bc3c91",
   "metadata": {},
   "source": [
    "**NOTE：** 下面单元格中的采样方式并没有被使用。因为他被后面的sample变量取代了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61f5c1-481c-4317-b563-b4c0deebf2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算正负样本的数量\n",
    "num_positive_samples = len(feature_data[feature_data['failure_tag'] == 1])\n",
    "num_negative_samples = len(feature_data[feature_data['failure_tag'] == 0])\n",
    "\n",
    "# 根据正样本的数量决定上采样倍数\n",
    "upsampling_factor = num_positive_samples // num_negative_samples\n",
    "\n",
    "# 负样本上采样\n",
    "sampled_negative_data = feature_data[feature_data['failure_tag'] == 0].sample(n=upsampling_factor, replace=True)\n",
    "\n",
    "# 合并正负样本\n",
    "upsampled_data = pd.concat([sampled_negative_data, feature_data[feature_data['failure_tag'] == 1]])\n",
    "\n",
    "# 重新采样后的数据集\n",
    "sample = upsampled_data\n",
    "\n",
    "# 将负样本的类别标签从1改为0\n",
    "upsampled_data['failure_tag'] = upsampled_data['failure_tag'].replace({1: 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49c4a5-258c-4fab-ac17-886cb0b74dea",
   "metadata": {},
   "source": [
    "**NOTE：** 下采样，比例为类别0的0.02.这里的sample最后被使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153575f3-2461-45bc-9400-d230c9d90726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负样本下采样\n",
    "sample_0 = feature_data[feature_data['failure_tag'] == 0].sample(frac=0.02)\n",
    "sample = pd.concat([sample_0, feature_data[feature_data['failure_tag'] == 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8893f9c6-cb72-448a-b603-40ac0295f198",
   "metadata": {},
   "source": [
    "**NOTE:** 下面所有构建的特征都没有使用，因为最后训练用的数据集使用的是sample，而这些特征都没有加入到sample中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572b55e-850a-4aa8-aa7e-286bab4951e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算异常值得分，将异常值得分作为特征，注意异常值检测特征的位置\n",
    "clf = IsolationForest(contamination=0.05)  # 设置异常值比例\n",
    "merged_data['outlier_score'] = clf.fit_predict(feature_data.iloc[:, :-1])\n",
    "\n",
    "# 时间特征：提取更多时间特征，如小时、分钟、周几等。\n",
    "merged_data['hour'] = merged_data['collect_time'].dt.hour\n",
    "merged_data['day_of_week'] = merged_data['collect_time'].dt.dayofweek\n",
    "\n",
    "# 将异常值得分作为特征\n",
    "feature_data['outlier_score'] = merged_data['outlier_score']\n",
    "\n",
    "# 在计算特征后插入异常值检测特征工程\n",
    "clf = IsolationForest(contamination=0.05)  # 设置异常值比例\n",
    "merged_data['outlier_score'] = clf.fit_predict(feature_data.iloc[:, :-1])\n",
    "\n",
    "# 将异常值得分作为特征\n",
    "feature_data['outlier_score'] = merged_data['outlier_score']\n",
    "\n",
    "# 过去故障次数\n",
    "merged_data['past_failure_count'] = merged_data.groupby('serial_number')['failure_tag'].cumsum().shift()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11efdd-d775-428c-91b1-05da5dacd4fa",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "- 这里加入了网格搜索，但是基本没用\n",
    "- 因为本数据集是严重倾斜的，而这里使用的交叉验证却是常规的交叉验证，如果要使用，应该使用分层交叉验证；如果你的采样比例是0.005，使用常规交叉验证或许是有效的\n",
    "- learning_rate默认为0.01\n",
    "- 使用分层交叉划分数据集不应该随机划分，因为该数据集包含时间序列\n",
    "- 尽管没什么用，但不会对最终结果造成太大影响\n",
    "- 模型未控制随机性，每次运行可能会有不同的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa17df-0f39-4303-bc00-2afb1f3dd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负样本上采样   #添加一个网格搜索代码\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(sample.iloc[:, :-1], sample['failure_tag'])\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# 使用最佳参数训练模型\n",
    "clf = XGBClassifier(**best_params)\n",
    "clf.fit(sample.iloc[:, :-1], sample['failure_tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdad82-f28a-49ac-ad37-0cf507e6f89a",
   "metadata": {},
   "source": [
    "**NOTE:** pti改成了5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851cfebf-26b1-473e-b55e-de8dd6027b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据\n",
    "group_data_test = etl(r'memory_sample_kernel_log_k12_round1_a_test.csv', AGG_TIME)\n",
    "group_min_sn_test = pd.DataFrame(group_data_test[['serial_number', 'collect_time']])\n",
    "group_min_test = group_data_test.drop(['serial_number', 'collect_time', 'manufacturer', 'vendor'], axis=1)\n",
    "\n",
    "# 模型预测\n",
    "res = clf.predict(group_min_test)\n",
    "group_min_sn_test['predict'] = res\n",
    "\n",
    "# 保存结果\n",
    "group_min_sn_test = group_min_sn_test[group_min_sn_test['predict'] == 1]\n",
    "group_min_sn_res = group_min_sn_test.drop('predict', axis=1)\n",
    "\n",
    "output_path = os.path.join('./', 'memory_predit_res_svm.csv')\n",
    "print(group_min_sn_res)\n",
    "\n",
    "# 由于预测pti对分数影响不大，先直接末尾增加pti为1\n",
    "pti = 5\n",
    "with open(output_path, 'w') as result_fp:\n",
    "    for _, _row in group_min_sn_res.iterrows():\n",
    "        result_fp.write(\"{},{},{}\\n\".format(_row.serial_number, _row.collect_time, pti))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e5159-72f7-448c-acd3-3a1d0b960407",
   "metadata": {},
   "source": [
    "## 特征重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537f0d1-b7ec-4249-8734-bfabb5bc655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 特征名称\n",
    "feature_names = sample.columns[:-1]  \n",
    "\n",
    "# 获取特征重要性并创建 DataFrame\n",
    "importances = clf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# 排序特征重要性\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 绘制特征重要性条形图\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # 反转Y轴，让重要性最高的特征显示在顶部\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73779d89-2e5c-489b-895d-dae6b1432635",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
